{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIKSSPmsdSBv",
        "outputId": "cca18801-1146-4ca9-ccb2-a6ea3e1d18b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 14 01:53:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    30W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZaTmalXu9P"
      },
      "source": [
        "# Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CgOuLZscXu9Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers as L\n",
        "import time\n",
        "from random import seed\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Am1DjEwXu9T"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHhLVMOXXu9T",
        "outputId": "82d69127-81cd-4349-88f0-f747fe658d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 18s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train/255.0\n",
        "x_valid = x_valid/255.0\n",
        "# x_train = np.expand_dims(x_train, axis=3)\n",
        "# x_valid = np.expand_dims(x_valid, axis=3)\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_valid = keras.utils.to_categorical(y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gwhPouIXu9U",
        "outputId": "7129e6cd-7b52-41ac-e6e6-1e0fa87a79b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 32, 3), 10)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "T_EPOCHS = 25\n",
        "S_EPOCHS = 20\n",
        "IMAGE_SIZE = x_train.shape[1:]\n",
        "BATCH_SIZE = 512\n",
        "N_CLASSES = y_train.shape[-1]\n",
        "IMAGE_SIZE, N_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UMxKQDlPXu9V"
      },
      "outputs": [],
      "source": [
        "def nn_callbacks():\n",
        "    es = keras.callbacks.EarlyStopping(\n",
        "        patience=5, verbose=1, restore_best_weights=True, min_delta=1e-4\n",
        "    )\n",
        "    rlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n",
        "    return [es, rlp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U7pEsC4yXu9X"
      },
      "outputs": [],
      "source": [
        "d_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "d_valid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
        "\n",
        "del x_train, x_valid, y_train, y_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFvFDVJdXu9Z"
      },
      "source": [
        "# Building the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwdwuyGoXu9Z"
      },
      "source": [
        "**Teacher Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyAinztKXu9a",
        "outputId": "0f838aaf-b1f9-4e14-edff-3072777c851c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 4s 0us/step\n",
            "Model: \"teacher\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,029,514\n",
            "Trainable params: 20,029,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_teacher_model(name='teacher'):\n",
        "    base_model = keras.applications.VGG19(input_shape=IMAGE_SIZE, include_top=False)\n",
        "    base_model.trainable = True\n",
        "    return keras.models.Sequential([\n",
        "            base_model,        \n",
        "            L.GlobalAvgPool2D(),        \n",
        "            L.Dense(N_CLASSES)\n",
        "        ], name=name\n",
        "    )\n",
        "        \n",
        "\n",
        "teacher_model = build_teacher_model()\n",
        "teacher_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMU7mOyCXu9a"
      },
      "source": [
        "**Student Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5caJ6TQGXu9a",
        "outputId": "4ee532f4-58d9-4ee0-ae2b-f782725f5e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"student\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 2, 2, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 1, 1, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 64)               0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 519,434\n",
            "Trainable params: 519,434\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_student_model(name='student'):\n",
        "    return keras.models.Sequential([\n",
        "        L.Conv2D(64, 3, input_shape=IMAGE_SIZE, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.MaxPool2D(pool_size=2),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.MaxPool2D(pool_size=2),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.MaxPool2D(pool_size=2),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.MaxPool2D(pool_size=2),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "        L.MaxPool2D(pool_size=2),\n",
        "        L.GlobalAvgPool2D(),\n",
        "        L.Dense(N_CLASSES),\n",
        "    ],name=name) \n",
        "\n",
        "student_model = build_student_model()\n",
        "student2_model = build_student_model()\n",
        "student4_model = build_student_model()\n",
        "student_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpa8I7bAXu9b"
      },
      "source": [
        "# Training Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcO1uQZXu9b",
        "outputId": "e3e00421-0544-4448-b80a-8a5f7ba1d1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "98/98 [==============================] - 42s 285ms/step - loss: 1.4579 - accuracy: 0.4877 - val_loss: 0.9621 - val_accuracy: 0.6646 - lr: 1.0000e-05\n",
            "Epoch 2/25\n",
            "98/98 [==============================] - 22s 221ms/step - loss: 0.8645 - accuracy: 0.6954 - val_loss: 0.8010 - val_accuracy: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 3/25\n",
            "98/98 [==============================] - 22s 223ms/step - loss: 0.7371 - accuracy: 0.7425 - val_loss: 0.7312 - val_accuracy: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 4/25\n",
            "98/98 [==============================] - 22s 224ms/step - loss: 0.6573 - accuracy: 0.7707 - val_loss: 0.6899 - val_accuracy: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 5/25\n",
            "98/98 [==============================] - 22s 221ms/step - loss: 0.5979 - accuracy: 0.7919 - val_loss: 0.6554 - val_accuracy: 0.7734 - lr: 1.0000e-05\n",
            "Epoch 6/25\n",
            "98/98 [==============================] - 22s 225ms/step - loss: 0.5477 - accuracy: 0.8107 - val_loss: 0.6462 - val_accuracy: 0.7780 - lr: 1.0000e-05\n",
            "Epoch 7/25\n",
            "98/98 [==============================] - 22s 224ms/step - loss: 0.5195 - accuracy: 0.8205 - val_loss: 0.6290 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
            "Epoch 8/25\n",
            "98/98 [==============================] - 22s 221ms/step - loss: 0.4838 - accuracy: 0.8311 - val_loss: 0.6118 - val_accuracy: 0.7885 - lr: 1.0000e-05\n",
            "Epoch 9/25\n",
            "98/98 [==============================] - 22s 225ms/step - loss: 0.4489 - accuracy: 0.8444 - val_loss: 0.5982 - val_accuracy: 0.7917 - lr: 1.0000e-05\n",
            "Epoch 10/25\n",
            "98/98 [==============================] - 22s 222ms/step - loss: 0.4235 - accuracy: 0.8543 - val_loss: 0.5879 - val_accuracy: 0.7996 - lr: 1.0000e-05\n",
            "Epoch 11/25\n",
            "98/98 [==============================] - 22s 223ms/step - loss: 0.3896 - accuracy: 0.8671 - val_loss: 0.5735 - val_accuracy: 0.8058 - lr: 1.0000e-05\n",
            "Epoch 12/25\n",
            "98/98 [==============================] - 22s 221ms/step - loss: 0.3591 - accuracy: 0.8771 - val_loss: 0.5764 - val_accuracy: 0.8061 - lr: 1.0000e-05\n",
            "Epoch 13/25\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8830\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "98/98 [==============================] - 22s 223ms/step - loss: 0.3420 - accuracy: 0.8830 - val_loss: 0.5778 - val_accuracy: 0.8078 - lr: 1.0000e-05\n",
            "Epoch 14/25\n",
            "98/98 [==============================] - 22s 224ms/step - loss: 0.2921 - accuracy: 0.9053 - val_loss: 0.5653 - val_accuracy: 0.8126 - lr: 1.0000e-06\n",
            "Epoch 15/25\n",
            "98/98 [==============================] - 22s 219ms/step - loss: 0.2835 - accuracy: 0.9067 - val_loss: 0.5669 - val_accuracy: 0.8136 - lr: 1.0000e-06\n",
            "Epoch 16/25\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9082\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "98/98 [==============================] - 22s 223ms/step - loss: 0.2788 - accuracy: 0.9082 - val_loss: 0.5689 - val_accuracy: 0.8136 - lr: 1.0000e-06\n",
            "Epoch 17/25\n",
            "98/98 [==============================] - 22s 222ms/step - loss: 0.2722 - accuracy: 0.9110 - val_loss: 0.5667 - val_accuracy: 0.8140 - lr: 1.0000e-07\n",
            "Epoch 18/25\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9120\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "98/98 [==============================] - 22s 224ms/step - loss: 0.2707 - accuracy: 0.9120 - val_loss: 0.5669 - val_accuracy: 0.8142 - lr: 1.0000e-07\n",
            "Epoch 19/25\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.9126Restoring model weights from the end of the best epoch: 14.\n",
            "98/98 [==============================] - 22s 220ms/step - loss: 0.2694 - accuracy: 0.9126 - val_loss: 0.5666 - val_accuracy: 0.8142 - lr: 1.0000e-08\n",
            "Epoch 19: early stopping\n"
          ]
        }
      ],
      "source": [
        "teacher_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5), \n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = teacher_model.fit(\n",
        "    d_train.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    epochs=T_EPOCHS,\n",
        "    callbacks=nn_callbacks(), \n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRFVCTs3_Sp"
      },
      "source": [
        "# Before Distill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F5S-ieAt4JIG"
      },
      "outputs": [],
      "source": [
        "distiller_Comp = False\n",
        "distiller2_Comp = False\n",
        "distiller4_Comp = False\n",
        "distiller4R_Comp = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XSisaGqXu9c"
      },
      "source": [
        "# Distillation in Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w55-SHa0Xu9c"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher, activation):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "        self.activation = activation\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=student_loss_fn)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                self.activation(teacher_predictions / self.temperature, axis=1),\n",
        "                self.activation(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        student_predictions = self.student(x, training=False)\n",
        "        \n",
        "        student_loss = self.student_loss_fn(y, student_predictions)\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            self.activation(teacher_predictions / self.temperature, axis=1),\n",
        "            self.activation(student_predictions / self.temperature, axis=1),\n",
        "        )\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        \n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "    \n",
        "    def call(self, x):\n",
        "        return self.student(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_qqbPXkXu9d",
        "outputId": "f5640f1b-2151-4847-8107-ce19ac23729f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "98/98 [==============================] - 22s 207ms/step - accuracy: 0.1505 - student_loss: 2.1984 - distillation_loss: 9.5399e-04 - loss: 1.5392 - val_accuracy: 0.2156 - val_student_loss: 2.0840 - val_distillation_loss: 9.0673e-04 - val_loss: 1.4591 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.3009 - student_loss: 1.7994 - distillation_loss: 7.0487e-04 - loss: 1.2598 - val_accuracy: 0.3775 - val_student_loss: 1.6514 - val_distillation_loss: 5.9116e-04 - val_loss: 1.1562 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.4293 - student_loss: 1.5156 - distillation_loss: 5.8337e-04 - loss: 1.0611 - val_accuracy: 0.4703 - val_student_loss: 1.4046 - val_distillation_loss: 5.7884e-04 - val_loss: 0.9834 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.5018 - student_loss: 1.3475 - distillation_loss: 5.2556e-04 - loss: 0.9434 - val_accuracy: 0.5236 - val_student_loss: 1.3891 - val_distillation_loss: 5.8871e-04 - val_loss: 0.9725 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - 19s 198ms/step - accuracy: 0.5629 - student_loss: 1.1902 - distillation_loss: 4.6293e-04 - loss: 0.8333 - val_accuracy: 0.5786 - val_student_loss: 1.1784 - val_distillation_loss: 4.8706e-04 - val_loss: 0.8250 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "98/98 [==============================] - 19s 197ms/step - accuracy: 0.6208 - student_loss: 1.0414 - distillation_loss: 4.0936e-04 - loss: 0.7291 - val_accuracy: 0.6196 - val_student_loss: 1.0936 - val_distillation_loss: 4.1703e-04 - val_loss: 0.7656 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.6644 - student_loss: 0.9283 - distillation_loss: 3.7796e-04 - loss: 0.6499 - val_accuracy: 0.6455 - val_student_loss: 1.1154 - val_distillation_loss: 3.9348e-04 - val_loss: 0.7809 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - 19s 197ms/step - accuracy: 0.6983 - student_loss: 0.8407 - distillation_loss: 3.5082e-04 - loss: 0.5886 - val_accuracy: 0.6753 - val_student_loss: 0.8854 - val_distillation_loss: 3.3153e-04 - val_loss: 0.6199 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "98/98 [==============================] - 19s 197ms/step - accuracy: 0.7286 - student_loss: 0.7629 - distillation_loss: 3.3010e-04 - loss: 0.5341 - val_accuracy: 0.6919 - val_student_loss: 0.8398 - val_distillation_loss: 3.0168e-04 - val_loss: 0.5879 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "98/98 [==============================] - 19s 197ms/step - accuracy: 0.7590 - student_loss: 0.6777 - distillation_loss: 3.0503e-04 - loss: 0.4745 - val_accuracy: 0.7068 - val_student_loss: 0.7711 - val_distillation_loss: 3.0485e-04 - val_loss: 0.5399 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.7796 - student_loss: 0.6297 - distillation_loss: 2.9998e-04 - loss: 0.4409 - val_accuracy: 0.7021 - val_student_loss: 0.8137 - val_distillation_loss: 3.3419e-04 - val_loss: 0.5697 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "98/98 [==============================] - 19s 197ms/step - accuracy: 0.7972 - student_loss: 0.5766 - distillation_loss: 2.9713e-04 - loss: 0.4037 - val_accuracy: 0.7356 - val_student_loss: 0.7097 - val_distillation_loss: 2.6886e-04 - val_loss: 0.4969 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.8157 - student_loss: 0.5217 - distillation_loss: 2.9318e-04 - loss: 0.3653 - val_accuracy: 0.7295 - val_student_loss: 0.7649 - val_distillation_loss: 3.1769e-04 - val_loss: 0.5355 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.8245 - student_loss: 0.4956 - distillation_loss: 3.0258e-04 - loss: 0.3470 - val_accuracy: 0.7431 - val_student_loss: 0.6830 - val_distillation_loss: 3.3787e-04 - val_loss: 0.4782 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.8448 - student_loss: 0.4400 - distillation_loss: 3.1593e-04 - loss: 0.3081 - val_accuracy: 0.7380 - val_student_loss: 0.8706 - val_distillation_loss: 3.5713e-04 - val_loss: 0.6095 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.8524 - student_loss: 0.4167 - distillation_loss: 3.2140e-04 - loss: 0.2918\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.8524 - student_loss: 0.4160 - distillation_loss: 3.2146e-04 - loss: 0.2913 - val_accuracy: 0.7339 - val_student_loss: 0.9787 - val_distillation_loss: 3.7037e-04 - val_loss: 0.6852 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.8998 - student_loss: 0.2905 - distillation_loss: 3.5383e-04 - loss: 0.2035 - val_accuracy: 0.7581 - val_student_loss: 0.9334 - val_distillation_loss: 4.2897e-04 - val_loss: 0.6535 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.9180 - student_loss: 0.2430 - distillation_loss: 4.3011e-04 - loss: 0.1703\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.9180 - student_loss: 0.2429 - distillation_loss: 4.3061e-04 - loss: 0.1702 - val_accuracy: 0.7585 - val_student_loss: 0.8925 - val_distillation_loss: 4.8271e-04 - val_loss: 0.6249 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.9305 - student_loss: 0.2127 - distillation_loss: 4.9756e-04 - loss: 0.1490Restoring model weights from the end of the best epoch: 14.\n",
            "98/98 [==============================] - 19s 196ms/step - accuracy: 0.9305 - student_loss: 0.2122 - distillation_loss: 4.9844e-04 - loss: 0.1487 - val_accuracy: 0.7597 - val_student_loss: 0.9484 - val_distillation_loss: 5.3719e-04 - val_loss: 0.6641 - lr: 1.0000e-05\n",
            "Epoch 19: early stopping\n"
          ]
        }
      ],
      "source": [
        "\n",
        "distiller = Distiller(student_model, teacher_model, tf.nn.softmax)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.7,\n",
        "    temperature=100,\n",
        ")\n",
        "history_distillation = distiller.fit(\n",
        "    d_train.shuffle(1024, 19).batch(BATCH_SIZE), \n",
        "    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    epochs=S_EPOCHS, callbacks=nn_callbacks(), batch_size=BATCH_SIZE\n",
        ")\n",
        "distiller_Comp = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeaODLJVZ9V4"
      },
      "source": [
        "# Distillation with 2 teachers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4VuOZUlDaPd4"
      },
      "outputs": [],
      "source": [
        "class Distiller2(keras.Model):\n",
        "    def __init__(self, student, teacher, teacher2,activation):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.teacher2 = teacher2\n",
        "        self.student = student\n",
        "        self.activation = activation\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=student_loss_fn)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher2_predictions = self.teacher2(x, training=False)\n",
        "\n",
        "\n",
        "        teacher_promedio = teacher_predictions*0.5 + teacher2_predictions*0.5\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                self.activation(teacher_promedio / self.temperature, axis=1),\n",
        "                self.activation(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher2_predictions = self.teacher2(x, training=False)\n",
        "\n",
        "        student_predictions = self.student(x, training=False)\n",
        "        \n",
        "        teacher_promedio = teacher_predictions*0.5 + teacher2_predictions*0.5\n",
        "        student_loss = self.student_loss_fn(y, student_predictions)\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            self.activation(teacher_promedio / self.temperature, axis=1),\n",
        "            self.activation(student_predictions / self.temperature, axis=1),\n",
        "        )\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        \n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "    \n",
        "    def call(self, x):\n",
        "        return self.student(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z999EAyJghlX",
        "outputId": "3f897a70-d595-4ba5-ac3a-ffe78a8430b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "98/98 [==============================] - 29s 283ms/step - accuracy: 0.1657 - student_loss: 2.2078 - distillation_loss: 9.7393e-04 - loss: 1.5457 - val_accuracy: 0.2893 - val_student_loss: 1.9762 - val_distillation_loss: 8.5065e-04 - val_loss: 1.3836 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.3344 - student_loss: 1.7911 - distillation_loss: 7.3545e-04 - loss: 1.2540 - val_accuracy: 0.3977 - val_student_loss: 1.6112 - val_distillation_loss: 6.3428e-04 - val_loss: 1.1280 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 27s 277ms/step - accuracy: 0.4241 - student_loss: 1.5550 - distillation_loss: 6.1060e-04 - loss: 1.0887 - val_accuracy: 0.4610 - val_student_loss: 1.4829 - val_distillation_loss: 5.9465e-04 - val_loss: 1.0382 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 27s 277ms/step - accuracy: 0.4916 - student_loss: 1.3868 - distillation_loss: 5.3886e-04 - loss: 0.9709 - val_accuracy: 0.4905 - val_student_loss: 1.3863 - val_distillation_loss: 5.9251e-04 - val_loss: 0.9706 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - 27s 280ms/step - accuracy: 0.5446 - student_loss: 1.2464 - distillation_loss: 4.8389e-04 - loss: 0.8727 - val_accuracy: 0.5526 - val_student_loss: 1.2834 - val_distillation_loss: 4.6828e-04 - val_loss: 0.8985 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "98/98 [==============================] - 27s 276ms/step - accuracy: 0.5982 - student_loss: 1.1105 - distillation_loss: 4.3752e-04 - loss: 0.7775 - val_accuracy: 0.5946 - val_student_loss: 1.2135 - val_distillation_loss: 4.2542e-04 - val_loss: 0.8496 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - 27s 279ms/step - accuracy: 0.6302 - student_loss: 1.0292 - distillation_loss: 4.1111e-04 - loss: 0.7206 - val_accuracy: 0.6227 - val_student_loss: 1.2217 - val_distillation_loss: 4.0152e-04 - val_loss: 0.8553 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.6639 - student_loss: 0.9371 - distillation_loss: 3.8236e-04 - loss: 0.6561 - val_accuracy: 0.6349 - val_student_loss: 1.0291 - val_distillation_loss: 3.7964e-04 - val_loss: 0.7205 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "98/98 [==============================] - 27s 277ms/step - accuracy: 0.6939 - student_loss: 0.8598 - distillation_loss: 3.5834e-04 - loss: 0.6020 - val_accuracy: 0.6513 - val_student_loss: 0.9525 - val_distillation_loss: 3.3216e-04 - val_loss: 0.6668 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "98/98 [==============================] - 27s 279ms/step - accuracy: 0.7193 - student_loss: 0.7876 - distillation_loss: 3.3388e-04 - loss: 0.5514 - val_accuracy: 0.6644 - val_student_loss: 0.8680 - val_distillation_loss: 3.1189e-04 - val_loss: 0.6077 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.7371 - student_loss: 0.7397 - distillation_loss: 3.2133e-04 - loss: 0.5179 - val_accuracy: 0.6642 - val_student_loss: 0.9972 - val_distillation_loss: 3.2414e-04 - val_loss: 0.6982 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.7617 - student_loss: 0.6719 - distillation_loss: 3.0604e-04 - loss: 0.4704\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.7617 - student_loss: 0.6704 - distillation_loss: 3.0593e-04 - loss: 0.4694 - val_accuracy: 0.6793 - val_student_loss: 0.9906 - val_distillation_loss: 3.0231e-04 - val_loss: 0.6935 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "98/98 [==============================] - 27s 276ms/step - accuracy: 0.8087 - student_loss: 0.5411 - distillation_loss: 2.7181e-04 - loss: 0.3788 - val_accuracy: 0.7063 - val_student_loss: 0.8997 - val_distillation_loss: 2.8975e-04 - val_loss: 0.6299 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.8295 - student_loss: 0.4877 - distillation_loss: 2.7026e-04 - loss: 0.3415 - val_accuracy: 0.7080 - val_student_loss: 0.8379 - val_distillation_loss: 3.0432e-04 - val_loss: 0.5866 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "98/98 [==============================] - 27s 278ms/step - accuracy: 0.8411 - student_loss: 0.4596 - distillation_loss: 2.7634e-04 - loss: 0.3218 - val_accuracy: 0.7052 - val_student_loss: 0.9127 - val_distillation_loss: 2.8263e-04 - val_loss: 0.6389 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.8502 - student_loss: 0.4356 - distillation_loss: 2.8565e-04 - loss: 0.3050\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "98/98 [==============================] - 27s 277ms/step - accuracy: 0.8502 - student_loss: 0.4358 - distillation_loss: 2.8561e-04 - loss: 0.3051 - val_accuracy: 0.7080 - val_student_loss: 1.0532 - val_distillation_loss: 3.1938e-04 - val_loss: 0.7373 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "98/98 [==============================] - 27s 279ms/step - accuracy: 0.8610 - student_loss: 0.4054 - distillation_loss: 2.9476e-04 - loss: 0.2839 - val_accuracy: 0.7128 - val_student_loss: 1.0052 - val_distillation_loss: 3.0742e-04 - val_loss: 0.7037 - lr: 1.0000e-05\n",
            "Epoch 18/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.8644 - student_loss: 0.3970 - distillation_loss: 2.9815e-04 - loss: 0.2780\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "98/98 [==============================] - 27s 277ms/step - accuracy: 0.8644 - student_loss: 0.3973 - distillation_loss: 2.9816e-04 - loss: 0.2782 - val_accuracy: 0.7123 - val_student_loss: 0.8606 - val_distillation_loss: 3.1787e-04 - val_loss: 0.6025 - lr: 1.0000e-05\n",
            "Epoch 19/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.8669 - student_loss: 0.3911 - distillation_loss: 2.9986e-04 - loss: 0.2739Restoring model weights from the end of the best epoch: 14.\n",
            "98/98 [==============================] - 27s 280ms/step - accuracy: 0.8669 - student_loss: 0.3908 - distillation_loss: 2.9983e-04 - loss: 0.2737 - val_accuracy: 0.7137 - val_student_loss: 0.9804 - val_distillation_loss: 3.2939e-04 - val_loss: 0.6864 - lr: 1.0000e-06\n",
            "Epoch 19: early stopping\n"
          ]
        }
      ],
      "source": [
        "student2_model = build_student_model()\n",
        "distiller2 = Distiller2(student2_model, teacher_model, teacher_model, tf.nn.softmax)\n",
        "distiller2.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.7,\n",
        "    temperature=100,\n",
        ")\n",
        "history_distillation = distiller2.fit(\n",
        "    d_train.shuffle(1024, 19).batch(BATCH_SIZE), \n",
        "    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    epochs=S_EPOCHS, callbacks=nn_callbacks(), batch_size=BATCH_SIZE\n",
        ")\n",
        "distiller2_Comp = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiKA7cxSocEt"
      },
      "source": [
        "# Distiller with 4 teachers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wxqDkioKogoK"
      },
      "outputs": [],
      "source": [
        "class Distiller4(keras.Model):\n",
        "    def __init__(self, student, teacher, teacher2,teacher3,teacher4,activation):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.teacher2 = teacher2\n",
        "        self.teacher3 = teacher3\n",
        "        self.teacher4 = teacher4\n",
        "        self.student = student\n",
        "        self.activation = activation\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=student_loss_fn)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher2_predictions = self.teacher2(x, training=False)\n",
        "        teacher3_predictions = self.teacher2(x, training=False)\n",
        "        teacher4_predictions = self.teacher2(x, training=False)\n",
        "\n",
        "\n",
        "        teacher_promedio = teacher_predictions*0.25 + teacher2_predictions*0.25 + teacher3_predictions*0.25 + teacher4_predictions*0.25\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                self.activation(teacher_promedio / self.temperature, axis=1),\n",
        "                self.activation(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        student_predictions = self.student(x, training=False)\n",
        "        \n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher2_predictions = self.teacher2(x, training=False)\n",
        "        teacher3_predictions = self.teacher2(x, training=False)\n",
        "        teacher4_predictions = self.teacher2(x, training=False)\n",
        "\n",
        "\n",
        "        teacher_promedio = teacher_predictions*0.25 + teacher2_predictions*0.25 + teacher3_predictions*0.25 + teacher4_predictions*0.25\n",
        "        student_loss = self.student_loss_fn(y, student_predictions)\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            self.activation(teacher_promedio / self.temperature, axis=1),\n",
        "            self.activation(student_predictions / self.temperature, axis=1),\n",
        "        )\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        \n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "    \n",
        "    def call(self, x):\n",
        "        return self.student(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG_gzWxOomsH",
        "outputId": "664b456b-7874-4779-adfb-a43d76a64b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "98/98 [==============================] - 45s 445ms/step - accuracy: 0.1235 - student_loss: 2.2649 - distillation_loss: 0.0010 - loss: 1.5857 - val_accuracy: 0.1700 - val_student_loss: 2.1704 - val_distillation_loss: 9.0170e-04 - val_loss: 1.5196 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 43s 442ms/step - accuracy: 0.2973 - student_loss: 1.8563 - distillation_loss: 7.5637e-04 - loss: 1.2996 - val_accuracy: 0.3456 - val_student_loss: 1.7238 - val_distillation_loss: 6.3737e-04 - val_loss: 1.2068 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 43s 441ms/step - accuracy: 0.4017 - student_loss: 1.5817 - distillation_loss: 6.1080e-04 - loss: 1.1074 - val_accuracy: 0.4597 - val_student_loss: 1.4618 - val_distillation_loss: 5.6360e-04 - val_loss: 1.0234 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 43s 438ms/step - accuracy: 0.4794 - student_loss: 1.3990 - distillation_loss: 5.4599e-04 - loss: 0.9794 - val_accuracy: 0.5082 - val_student_loss: 1.3832 - val_distillation_loss: 5.8836e-04 - val_loss: 0.9685 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - 43s 442ms/step - accuracy: 0.5432 - student_loss: 1.2410 - distillation_loss: 4.8496e-04 - loss: 0.8689 - val_accuracy: 0.5572 - val_student_loss: 1.1506 - val_distillation_loss: 5.0418e-04 - val_loss: 0.8056 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "98/98 [==============================] - 43s 437ms/step - accuracy: 0.5938 - student_loss: 1.1101 - distillation_loss: 4.3652e-04 - loss: 0.7772 - val_accuracy: 0.5930 - val_student_loss: 1.1849 - val_distillation_loss: 4.4594e-04 - val_loss: 0.8296 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.6353 - student_loss: 1.0064 - distillation_loss: 4.0136e-04 - loss: 0.7046\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "98/98 [==============================] - 43s 438ms/step - accuracy: 0.6353 - student_loss: 1.0052 - distillation_loss: 4.0124e-04 - loss: 0.7038 - val_accuracy: 0.6291 - val_student_loss: 1.1954 - val_distillation_loss: 4.1609e-04 - val_loss: 0.8369 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - 43s 440ms/step - accuracy: 0.6931 - student_loss: 0.8504 - distillation_loss: 3.4610e-04 - loss: 0.5954 - val_accuracy: 0.6667 - val_student_loss: 0.9104 - val_distillation_loss: 3.3556e-04 - val_loss: 0.6374 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "98/98 [==============================] - 43s 442ms/step - accuracy: 0.7078 - student_loss: 0.8115 - distillation_loss: 3.2693e-04 - loss: 0.5682 - val_accuracy: 0.6752 - val_student_loss: 0.9032 - val_distillation_loss: 3.2483e-04 - val_loss: 0.6323 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "98/98 [==============================] - 43s 441ms/step - accuracy: 0.7151 - student_loss: 0.7871 - distillation_loss: 3.1935e-04 - loss: 0.5510 - val_accuracy: 0.6834 - val_student_loss: 0.8515 - val_distillation_loss: 3.0828e-04 - val_loss: 0.5961 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "98/98 [==============================] - 43s 441ms/step - accuracy: 0.7247 - student_loss: 0.7651 - distillation_loss: 3.1324e-04 - loss: 0.5356 - val_accuracy: 0.6877 - val_student_loss: 0.9005 - val_distillation_loss: 3.1074e-04 - val_loss: 0.6305 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "98/98 [==============================] - 43s 440ms/step - accuracy: 0.7316 - student_loss: 0.7421 - distillation_loss: 3.0751e-04 - loss: 0.5195 - val_accuracy: 0.6901 - val_student_loss: 0.8341 - val_distillation_loss: 3.0641e-04 - val_loss: 0.5840 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "98/98 [==============================] - 43s 442ms/step - accuracy: 0.7383 - student_loss: 0.7279 - distillation_loss: 3.0435e-04 - loss: 0.5096 - val_accuracy: 0.6967 - val_student_loss: 0.7484 - val_distillation_loss: 2.9808e-04 - val_loss: 0.5239 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "98/98 [==============================] - 43s 438ms/step - accuracy: 0.7462 - student_loss: 0.7058 - distillation_loss: 3.0085e-04 - loss: 0.4941 - val_accuracy: 0.7010 - val_student_loss: 0.7818 - val_distillation_loss: 3.1864e-04 - val_loss: 0.5474 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.7522 - student_loss: 0.6918 - distillation_loss: 2.9940e-04 - loss: 0.4843\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "98/98 [==============================] - 43s 441ms/step - accuracy: 0.7522 - student_loss: 0.6915 - distillation_loss: 2.9927e-04 - loss: 0.4841 - val_accuracy: 0.7063 - val_student_loss: 0.8642 - val_distillation_loss: 3.0792e-04 - val_loss: 0.6050 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "98/98 [==============================] - 43s 441ms/step - accuracy: 0.7641 - student_loss: 0.6639 - distillation_loss: 2.9362e-04 - loss: 0.4648 - val_accuracy: 0.7087 - val_student_loss: 0.8682 - val_distillation_loss: 2.8412e-04 - val_loss: 0.6079 - lr: 1.0000e-05\n",
            "Epoch 17/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.7664 - student_loss: 0.6591 - distillation_loss: 2.9242e-04 - loss: 0.4615\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "98/98 [==============================] - 43s 438ms/step - accuracy: 0.7664 - student_loss: 0.6595 - distillation_loss: 2.9239e-04 - loss: 0.4618 - val_accuracy: 0.7093 - val_student_loss: 0.9063 - val_distillation_loss: 2.8710e-04 - val_loss: 0.6345 - lr: 1.0000e-05\n",
            "Epoch 18/20\n",
            "98/98 [==============================] - ETA: 0s - accuracy: 0.7681 - student_loss: 0.6546 - distillation_loss: 2.9147e-04 - loss: 0.4583Restoring model weights from the end of the best epoch: 13.\n",
            "98/98 [==============================] - 43s 439ms/step - accuracy: 0.7681 - student_loss: 0.6540 - distillation_loss: 2.9130e-04 - loss: 0.4579 - val_accuracy: 0.7089 - val_student_loss: 0.8588 - val_distillation_loss: 2.9365e-04 - val_loss: 0.6012 - lr: 1.0000e-06\n",
            "Epoch 18: early stopping\n"
          ]
        }
      ],
      "source": [
        "student4_model = build_student_model()\n",
        "distiller4 = Distiller4(student4_model, teacher_model, teacher_model,teacher_model,teacher_model, tf.nn.softmax)\n",
        "distiller4.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.7,\n",
        "    temperature=100,\n",
        ")\n",
        "history_distillation = distiller4.fit(\n",
        "    d_train.shuffle(1024, 19).batch(BATCH_SIZE), \n",
        "    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    epochs=S_EPOCHS, callbacks=nn_callbacks(), batch_size=BATCH_SIZE\n",
        ")\n",
        "distiller4_Comp = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFafEvSazGwB"
      },
      "source": [
        "# Distiller with 4 teachers random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kuAUSl22zJ7k"
      },
      "outputs": [],
      "source": [
        "class Distiller4R(keras.Model):\n",
        "    def __init__(self, student, teacher, teacher2,teacher3,teacher4, activation):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.teacher2 = teacher2\n",
        "        self.teacher3 = teacher3\n",
        "        self.teacher4 = teacher4\n",
        "        self.student = student\n",
        "        self.activation = activation\n",
        "        self.value = randint(0, 1000)%4\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "        \n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=student_loss_fn)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        if (self.value==0):\n",
        "          teacher_predictions = self.teacher(x, training=False)\n",
        "        elif(self.value==1):\n",
        "          teacher_predictions = self.teacher2(x, training=False)\n",
        "        elif(self.value==2):\n",
        "          teacher_predictions = self.teacher3(x, training=False)\n",
        "        else:\n",
        "          teacher_predictions = self.teacher4(x, training=False)\n",
        "\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                self.activation(teacher_predictions / self.temperature, axis=1),\n",
        "                self.activation(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        self.value = randint(0, 1000)%4\n",
        "        x, y = data\n",
        "        if(self.value ==0):\n",
        "          teacher_predictions = self.teacher(x, training=False)\n",
        "        elif(self.value ==1):\n",
        "          teacher_predictions = self.teacher2(x, training=False)\n",
        "        elif(self.value ==2):\n",
        "          teacher_predictions = self.teacher3(x, training=False)\n",
        "        else:\n",
        "          teacher_predictions = self.teacher4(x, training=False)   \n",
        "\n",
        "        student_predictions = self.student(x, training=False)\n",
        "        student_loss = self.student_loss_fn(y, student_predictions)\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            self.activation(teacher_predictions / self.temperature, axis=1),\n",
        "            self.activation(student_predictions / self.temperature, axis=1),\n",
        "        )\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        \n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n",
        "        )\n",
        "        return results\n",
        "    \n",
        "    def call(self, x):\n",
        "        return self.student(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jKULSFdb3CRU"
      },
      "outputs": [],
      "source": [
        "student4R_model = build_student_model()\n",
        "distiller4R = Distiller4R(student4R_model, teacher_model, teacher_model,teacher_model,teacher_model, tf.nn.softmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XcQ3M_8jJPFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685a1f57-69b8-4aa8-8b5e-5df487e9d168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "98/98 [==============================] - 21s 199ms/step - accuracy: 0.2206 - student_loss: 2.0802 - distillation_loss: 9.0966e-04 - loss: 1.4564 - val_accuracy: 0.2556 - val_student_loss: 1.9855 - val_distillation_loss: 8.8651e-04 - val_loss: 1.3901 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.3316 - student_loss: 1.7799 - distillation_loss: 7.2311e-04 - loss: 1.2462 - val_accuracy: 0.3814 - val_student_loss: 1.6255 - val_distillation_loss: 6.5882e-04 - val_loss: 1.1380 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.3509 - student_loss: 1.7496 - distillation_loss: 6.9236e-04 - loss: 1.2250 - val_accuracy: 0.4125 - val_student_loss: 1.6101 - val_distillation_loss: 6.2207e-04 - val_loss: 1.1272 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.3455 - student_loss: 1.8430 - distillation_loss: 7.3133e-04 - loss: 1.2903 - val_accuracy: 0.4079 - val_student_loss: 1.6524 - val_distillation_loss: 6.0226e-04 - val_loss: 1.1569 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - 21s 201ms/step - accuracy: 0.3618 - student_loss: 1.7767 - distillation_loss: 7.1892e-04 - loss: 1.2439 - val_accuracy: 0.4507 - val_student_loss: 1.5591 - val_distillation_loss: 6.0903e-04 - val_loss: 1.0915 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "98/98 [==============================] - 21s 201ms/step - accuracy: 0.3750 - student_loss: 1.7199 - distillation_loss: 6.9286e-04 - loss: 1.2042 - val_accuracy: 0.4828 - val_student_loss: 1.4565 - val_distillation_loss: 5.4804e-04 - val_loss: 1.0197 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.4400 - student_loss: 1.5079 - distillation_loss: 5.7496e-04 - loss: 1.0557 - val_accuracy: 0.5096 - val_student_loss: 1.4136 - val_distillation_loss: 4.9628e-04 - val_loss: 0.9897 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.4060 - student_loss: 1.6539 - distillation_loss: 6.4300e-04 - loss: 1.1579 - val_accuracy: 0.5079 - val_student_loss: 1.4819 - val_distillation_loss: 5.1278e-04 - val_loss: 1.0375 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.4222 - student_loss: 1.6580 - distillation_loss: 6.7126e-04 - loss: 1.1608 - val_accuracy: 0.5198 - val_student_loss: 1.3650 - val_distillation_loss: 4.9443e-04 - val_loss: 0.9557 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "98/98 [==============================] - 21s 199ms/step - accuracy: 0.4911 - student_loss: 1.3940 - distillation_loss: 5.2986e-04 - loss: 0.9760 - val_accuracy: 0.5549 - val_student_loss: 1.3211 - val_distillation_loss: 4.3576e-04 - val_loss: 0.9249 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.5465 - student_loss: 1.2265 - distillation_loss: 4.6279e-04 - loss: 0.8587 - val_accuracy: 0.5730 - val_student_loss: 1.2398 - val_distillation_loss: 4.0909e-04 - val_loss: 0.8680 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.5549 - student_loss: 1.2274 - distillation_loss: 4.7641e-04 - loss: 0.8593 - val_accuracy: 0.6032 - val_student_loss: 1.1669 - val_distillation_loss: 4.0222e-04 - val_loss: 0.8169 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.5587 - student_loss: 1.2271 - distillation_loss: 4.8297e-04 - loss: 0.8591 - val_accuracy: 0.6146 - val_student_loss: 1.1465 - val_distillation_loss: 4.2169e-04 - val_loss: 0.8027 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.5916 - student_loss: 1.1424 - distillation_loss: 4.4910e-04 - loss: 0.7998 - val_accuracy: 0.6401 - val_student_loss: 1.0836 - val_distillation_loss: 3.6905e-04 - val_loss: 0.7587 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.6173 - student_loss: 1.0781 - distillation_loss: 4.3155e-04 - loss: 0.7548 - val_accuracy: 0.6566 - val_student_loss: 1.0232 - val_distillation_loss: 3.6895e-04 - val_loss: 0.7164 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "98/98 [==============================] - 21s 201ms/step - accuracy: 0.6423 - student_loss: 1.0099 - distillation_loss: 4.1819e-04 - loss: 0.7070 - val_accuracy: 0.6652 - val_student_loss: 1.0103 - val_distillation_loss: 3.5841e-04 - val_loss: 0.7073 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.6670 - student_loss: 0.9379 - distillation_loss: 3.8462e-04 - loss: 0.6567 - val_accuracy: 0.6828 - val_student_loss: 0.9572 - val_distillation_loss: 3.5083e-04 - val_loss: 0.6702 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.6742 - student_loss: 0.9314 - distillation_loss: 3.9591e-04 - loss: 0.6521 - val_accuracy: 0.6859 - val_student_loss: 0.9622 - val_distillation_loss: 3.6762e-04 - val_loss: 0.6737 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "98/98 [==============================] - 21s 200ms/step - accuracy: 0.6722 - student_loss: 0.9253 - distillation_loss: 3.8631e-04 - loss: 0.6478 - val_accuracy: 0.6879 - val_student_loss: 0.9119 - val_distillation_loss: 3.5147e-04 - val_loss: 0.6384 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "98/98 [==============================] - 21s 201ms/step - accuracy: 0.7119 - student_loss: 0.8233 - distillation_loss: 3.6502e-04 - loss: 0.5764 - val_accuracy: 0.6935 - val_student_loss: 0.9005 - val_distillation_loss: 3.3306e-04 - val_loss: 0.6305 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "random_epoch=20\n",
        "for i in range(random_epoch):\n",
        "    print(\"Epoch \" +str(i+1) +\"/\"+str(random_epoch))\n",
        "    distiller4R.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.7,\n",
        "    temperature=100,\n",
        "    )\n",
        "    if (i!=0):\n",
        "      student4R_model.set_weights(weights)\n",
        "    history_distillation = distiller4R.fit(\n",
        "    d_train.shuffle(1024, 19).batch(BATCH_SIZE), \n",
        "    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n",
        "    epochs=1, callbacks=[nn_callbacks()], batch_size=BATCH_SIZE)\n",
        "    weights = student4R_model.get_weights()\n",
        "distiller4R_Comp= True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTg5KalOXu9d"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jwA8CFx7Xu9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5077ee-4d13-4b56-d50a-e9213ec8761c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model:\n",
            "20/20 [==============================] - 2s 75ms/step - loss: 0.5653 - accuracy: 0.8126\n",
            "File Size is : 229.35 MB\n",
            "Distilled Model:\n",
            "20/20 [==============================] - 1s 42ms/step - loss: 0.8410 - accuracy: 0.7431\n",
            "File Size is : 6.09 MB\n",
            "Distilled Model 2:\n",
            "20/20 [==============================] - 1s 43ms/step - loss: 0.9188 - accuracy: 0.7080\n",
            "File Size is : 6.09 MB\n",
            "Distilled Model 4:\n",
            "20/20 [==============================] - 1s 42ms/step - loss: 0.8699 - accuracy: 0.6967\n",
            "File Size is : 6.09 MB\n",
            "Distilled Model 4R:\n",
            "20/20 [==============================] - 1s 43ms/step - loss: 0.8734 - accuracy: 0.6935\n",
            "File Size is : 6.09 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print('Teacher Model:')\n",
        "teacher_model.save('teacher.h5')\n",
        "teacher_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\n",
        "print(\"File Size is :\", round(os.path.getsize('teacher.h5')/1024**2, 2), \"MB\")\n",
        "\n",
        "if(distiller_Comp):\n",
        "  print('Distilled Model:')\n",
        "  student_model.save('student.h5')\n",
        "  student_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\n",
        "  print(\"File Size is :\", round(os.path.getsize('student.h5')/1024**2, 2), \"MB\")\n",
        "\n",
        "if(distiller2_Comp):\n",
        "  print('Distilled Model 2:')\n",
        "  student2_model.save('student2.h5')\n",
        "  student2_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\n",
        "  print(\"File Size is :\", round(os.path.getsize('student2.h5')/1024**2, 2), \"MB\")\n",
        "\n",
        "if(distiller4_Comp):\n",
        "  print('Distilled Model 4:')\n",
        "  student4_model.save('student4.h5')\n",
        "  student4_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\n",
        "  print(\"File Size is :\", round(os.path.getsize('student4.h5')/1024**2, 2), \"MB\")\n",
        "\n",
        "if(distiller4R_Comp):\n",
        "  print('Distilled Model 4R:')\n",
        "  student4R_model.save('student4R.h5')\n",
        "  student4R_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\n",
        "  print(\"File Size is :\", round(os.path.getsize('student4R.h5')/1024**2, 2), \"MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN0io6NTXu9d"
      },
      "source": [
        "**Reference**\n",
        "\n",
        "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
        "* [Implementation of classical Knowledge Distillation](https://keras.io/examples/vision/knowledge_distillation/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4l6S3ZNdQbO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}